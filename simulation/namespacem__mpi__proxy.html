<!-- HTML header for doxygen 1.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>MFC: Simulation: m_mpi_proxy Module Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
// This file is set as MATHJAX_CODEFILE in the Doxyfile. It configures how
// MathJax renders expressions in Markdown so that it is consistent with GitHub.
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath:  [ ['$',  '$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: "line" // Ignore code blocks: https://web.archive.org/web/20120430100225/http://www.mathjax.org/docs/1.1/options/tex2jax.html
    },
    "HTML-CSS": {
      fonts: ["TeX"]
    }
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" href="icon.ico" type="image/x-icon" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="icon.ico"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">MFC: Simulation
   </div>
   <div id="projectbrief">High-fidelity multiphase flow simulation</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('namespacem__mpi__proxy.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#func-members">Functions/Subroutines</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">m_mpi_proxy Module Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>The module serves as a proxy to the parameters and subroutines available in the MPI implementation's MPI module. Specifically, the purpose of the proxy is to harness basic MPI commands into more complicated procedures as to accomplish the communication goals for the simulation.  
<a href="namespacem__mpi__proxy.html#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions/Subroutines</h2></td></tr>
<tr class="memitem:a9bc4c617505152d3cc553e5bc25c1ee1"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a9bc4c617505152d3cc553e5bc25c1ee1">s_mpi_initialize</a> ()</td></tr>
<tr class="memdesc:a9bc4c617505152d3cc553e5bc25c1ee1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine intializes the MPI execution environment and queries both the number of processors which will be available for the job and the local processor rank.  <a href="namespacem__mpi__proxy.html#a9bc4c617505152d3cc553e5bc25c1ee1">More...</a><br /></td></tr>
<tr class="separator:a9bc4c617505152d3cc553e5bc25c1ee1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04ac565bad2b22dc045a5eeb4f516e2e"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a04ac565bad2b22dc045a5eeb4f516e2e">s_mpi_abort</a> ()</td></tr>
<tr class="memdesc:a04ac565bad2b22dc045a5eeb4f516e2e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine terminates the MPI execution environment.  <a href="namespacem__mpi__proxy.html#a04ac565bad2b22dc045a5eeb4f516e2e">More...</a><br /></td></tr>
<tr class="separator:a04ac565bad2b22dc045a5eeb4f516e2e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ff35ede51e90c483969e44c31303415"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a2ff35ede51e90c483969e44c31303415">s_initialize_mpi_data</a> (q_cons_vf)</td></tr>
<tr class="memdesc:a2ff35ede51e90c483969e44c31303415"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine that initializes MPI data structures.  <a href="namespacem__mpi__proxy.html#a2ff35ede51e90c483969e44c31303415">More...</a><br /></td></tr>
<tr class="separator:a2ff35ede51e90c483969e44c31303415"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abfbc42cea69273bc9fa4a2d78f636eb1"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#abfbc42cea69273bc9fa4a2d78f636eb1">s_mpi_barrier</a> ()</td></tr>
<tr class="memdesc:abfbc42cea69273bc9fa4a2d78f636eb1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Halts all processes until all have reached barrier.  <a href="namespacem__mpi__proxy.html#abfbc42cea69273bc9fa4a2d78f636eb1">More...</a><br /></td></tr>
<tr class="separator:abfbc42cea69273bc9fa4a2d78f636eb1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a015ee2c0892e9cfcb858da8f27b646d5"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a015ee2c0892e9cfcb858da8f27b646d5">s_initialize_mpi_proxy_module</a> ()</td></tr>
<tr class="memdesc:a015ee2c0892e9cfcb858da8f27b646d5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The computation of parameters, the allocation of memory, the association of pointers and/or the execution of any other procedures that are necessary to setup the module.  <a href="namespacem__mpi__proxy.html#a015ee2c0892e9cfcb858da8f27b646d5">More...</a><br /></td></tr>
<tr class="separator:a015ee2c0892e9cfcb858da8f27b646d5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a69660c5fe9302a8c0496b622fa3b5286"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a69660c5fe9302a8c0496b622fa3b5286">s_mpi_bcast_user_inputs</a> ()</td></tr>
<tr class="memdesc:a69660c5fe9302a8c0496b622fa3b5286"><td class="mdescLeft">&#160;</td><td class="mdescRight">Since only the processor with rank 0 reads and verifies the consistency of user inputs, these are initially not available to the other processors. Then, the purpose of this subroutine is to distribute the user inputs to the remaining processors in the communicator.  <a href="namespacem__mpi__proxy.html#a69660c5fe9302a8c0496b622fa3b5286">More...</a><br /></td></tr>
<tr class="separator:a69660c5fe9302a8c0496b622fa3b5286"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99b8eceb980f90faedef19fb4835e434"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a99b8eceb980f90faedef19fb4835e434">mpi_bcast_time_step_values</a> (proc_time, time_avg)</td></tr>
<tr class="separator:a99b8eceb980f90faedef19fb4835e434"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80c5e235786545276fe6ffa06965017f"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a80c5e235786545276fe6ffa06965017f">s_mpi_decompose_computational_domain</a> ()</td></tr>
<tr class="memdesc:a80c5e235786545276fe6ffa06965017f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The purpose of this procedure is to optimally decompose the computational domain among the available processors. This is performed by attempting to award each processor, in each of the coordinate directions, approximately the same number of cells, and then recomputing the affected global parameters.  <a href="namespacem__mpi__proxy.html#a80c5e235786545276fe6ffa06965017f">More...</a><br /></td></tr>
<tr class="separator:a80c5e235786545276fe6ffa06965017f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac348cb6f02f2a9ab70d5c7eac6320231"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#ac348cb6f02f2a9ab70d5c7eac6320231">s_mpi_sendrecv_grid_variables_buffers</a> (mpi_dir, pbc_loc)</td></tr>
<tr class="memdesc:ac348cb6f02f2a9ab70d5c7eac6320231"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this procedure is to populate the buffers of the grid variables by communicating with the neighboring processors. Note that only the buffers of the cell-width distributions are handled in such a way. This is because the buffers of cell-boundary locations may be calculated directly from those of the cell-width distributions.  <a href="namespacem__mpi__proxy.html#ac348cb6f02f2a9ab70d5c7eac6320231">More...</a><br /></td></tr>
<tr class="separator:ac348cb6f02f2a9ab70d5c7eac6320231"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a340a2c9b1688582ee485dd5d9bbcc864"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a340a2c9b1688582ee485dd5d9bbcc864">s_mpi_reduce_stability_criteria_extrema</a> (icfl_max_loc, vcfl_max_loc, ccfl_max_loc, Rc_min_loc, icfl_max_glb, vcfl_max_glb, ccfl_max_glb, Rc_min_glb)</td></tr>
<tr class="memdesc:a340a2c9b1688582ee485dd5d9bbcc864"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this subroutine is to determine the global extrema of the stability criteria in the computational domain. This is performed by sifting through the local extrema of each stability criterion. Note that each of the local extrema is from a single process, within its assigned section of the computational domain. Finally, note that the global extrema values are only bookkeept on the rank 0 processor.  <a href="namespacem__mpi__proxy.html#a340a2c9b1688582ee485dd5d9bbcc864">More...</a><br /></td></tr>
<tr class="separator:a340a2c9b1688582ee485dd5d9bbcc864"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a873520062481f3dabcfa77f9bd7728d3"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a873520062481f3dabcfa77f9bd7728d3">s_mpi_allreduce_sum</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a873520062481f3dabcfa77f9bd7728d3"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <a href="namespacem__mpi__proxy.html#a873520062481f3dabcfa77f9bd7728d3">More...</a><br /></td></tr>
<tr class="separator:a873520062481f3dabcfa77f9bd7728d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a398a372625502c0430a64e0bb23e2342"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a398a372625502c0430a64e0bb23e2342">s_mpi_allreduce_min</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a398a372625502c0430a64e0bb23e2342"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the minimum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <a href="namespacem__mpi__proxy.html#a398a372625502c0430a64e0bb23e2342">More...</a><br /></td></tr>
<tr class="separator:a398a372625502c0430a64e0bb23e2342"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff7286de058c49b2a5d7a84be1983e8f"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#aff7286de058c49b2a5d7a84be1983e8f">s_mpi_allreduce_max</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:aff7286de058c49b2a5d7a84be1983e8f"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the maximum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <a href="namespacem__mpi__proxy.html#aff7286de058c49b2a5d7a84be1983e8f">More...</a><br /></td></tr>
<tr class="separator:aff7286de058c49b2a5d7a84be1983e8f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9015d32f3d27c21a75e8a916e5090b96"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a9015d32f3d27c21a75e8a916e5090b96">s_mpi_sendrecv_conservative_variables_buffers</a> (q_cons_vf, mpi_dir, pbc_loc)</td></tr>
<tr class="memdesc:a9015d32f3d27c21a75e8a916e5090b96"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this procedure is to populate the buffers of the cell-average conservative variables by communicating with the neighboring processors.  <a href="namespacem__mpi__proxy.html#a9015d32f3d27c21a75e8a916e5090b96">More...</a><br /></td></tr>
<tr class="separator:a9015d32f3d27c21a75e8a916e5090b96"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac984c84fe4140876d6600250af9807da"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#ac984c84fe4140876d6600250af9807da">s_finalize_mpi_proxy_module</a> ()</td></tr>
<tr class="memdesc:ac984c84fe4140876d6600250af9807da"><td class="mdescLeft">&#160;</td><td class="mdescRight">Module deallocation and/or disassociation procedures.  <a href="namespacem__mpi__proxy.html#ac984c84fe4140876d6600250af9807da">More...</a><br /></td></tr>
<tr class="separator:ac984c84fe4140876d6600250af9807da"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a43fbda10c02ec8bc1fc572c83090f2e5"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a43fbda10c02ec8bc1fc572c83090f2e5">s_mpi_finalize</a> ()</td></tr>
<tr class="memdesc:a43fbda10c02ec8bc1fc572c83090f2e5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine finalizes the MPI execution environment.  <a href="namespacem__mpi__proxy.html#a43fbda10c02ec8bc1fc572c83090f2e5">More...</a><br /></td></tr>
<tr class="separator:a43fbda10c02ec8bc1fc572c83090f2e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a67f9b5b69858f72c61b8d8ebf212567d"><td class="memItemLeft" align="right" valign="top">real(kind(0d0)), dimension(:), allocatable, private&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a67f9b5b69858f72c61b8d8ebf212567d">q_cons_buff_send</a></td></tr>
<tr class="memdesc:a67f9b5b69858f72c61b8d8ebf212567d"><td class="mdescLeft">&#160;</td><td class="mdescRight">This variable is utilized to pack and send the buffer of the cell-average conservative variables, for a single computational domain boundary at the time, to the relevant neighboring processor.  <a href="namespacem__mpi__proxy.html#a67f9b5b69858f72c61b8d8ebf212567d">More...</a><br /></td></tr>
<tr class="separator:a67f9b5b69858f72c61b8d8ebf212567d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2915932883654eecbac1283ce03be3ba"><td class="memItemLeft" align="right" valign="top">real(kind(0d0)), dimension(:), allocatable, private&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacem__mpi__proxy.html#a2915932883654eecbac1283ce03be3ba">q_cons_buff_recv</a></td></tr>
<tr class="memdesc:a2915932883654eecbac1283ce03be3ba"><td class="mdescLeft">&#160;</td><td class="mdescRight">q_cons_buff_recv is utilized to receive and unpack the buffer of the cell- average conservative variables, for a single computational domain boundary at the time, from the relevant neighboring processor.  <a href="namespacem__mpi__proxy.html#a2915932883654eecbac1283ce03be3ba">More...</a><br /></td></tr>
<tr class="separator:a2915932883654eecbac1283ce03be3ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>The module serves as a proxy to the parameters and subroutines available in the MPI implementation's MPI module. Specifically, the purpose of the proxy is to harness basic MPI commands into more complicated procedures as to accomplish the communication goals for the simulation. </p>
</div><h2 class="groupheader">Function/Subroutine Documentation</h2>
<a id="a99b8eceb980f90faedef19fb4835e434"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a99b8eceb980f90faedef19fb4835e434">&#9670;&nbsp;</a></span>mpi_bcast_time_step_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::mpi_bcast_time_step_values </td>
          <td>(</td>
          <td class="paramtype">real(kind(0d0)), dimension(0:num_procs - 1), intent(inout)&#160;</td>
          <td class="paramname"><em>proc_time</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(inout)&#160;</td>
          <td class="paramname"><em>time_avg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a99b8eceb980f90faedef19fb4835e434_icgraph.svg" width="315" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="ac984c84fe4140876d6600250af9807da"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac984c84fe4140876d6600250af9807da">&#9670;&nbsp;</a></span>s_finalize_mpi_proxy_module()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_finalize_mpi_proxy_module</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Module deallocation and/or disassociation procedures. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_ac984c84fe4140876d6600250af9807da_icgraph.svg" width="308" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a2ff35ede51e90c483969e44c31303415"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ff35ede51e90c483969e44c31303415">&#9670;&nbsp;</a></span>s_initialize_mpi_data()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_initialize_mpi_data </td>
          <td>(</td>
          <td class="paramtype">type(scalar_field), dimension(sys_size), intent(in)&#160;</td>
          <td class="paramname"><em>q_cons_vf</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The subroutine that initializes MPI data structures. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">q_cons_vf</td><td>Conservative variables </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a2ff35ede51e90c483969e44c31303415_icgraph.svg" width="100%" height="448"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="a015ee2c0892e9cfcb858da8f27b646d5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a015ee2c0892e9cfcb858da8f27b646d5">&#9670;&nbsp;</a></span>s_initialize_mpi_proxy_module()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_initialize_mpi_proxy_module</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The computation of parameters, the allocation of memory, the association of pointers and/or the execution of any other procedures that are necessary to setup the module. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a015ee2c0892e9cfcb858da8f27b646d5_icgraph.svg" width="318" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a04ac565bad2b22dc045a5eeb4f516e2e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a04ac565bad2b22dc045a5eeb4f516e2e">&#9670;&nbsp;</a></span>s_mpi_abort()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_abort</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The subroutine terminates the MPI execution environment. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a04ac565bad2b22dc045a5eeb4f516e2e_icgraph.svg" width="100%" height="600"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="aff7286de058c49b2a5d7a84be1983e8f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff7286de058c49b2a5d7a84be1983e8f">&#9670;&nbsp;</a></span>s_mpi_allreduce_max()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_allreduce_max </td>
          <td>(</td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>var_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>var_glb</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The following subroutine takes the input local variable from all processors and reduces to the maximum of all values. The reduced variable is recorded back onto the original local variable on each processor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">var_loc</td><td>Some variable containing the local value which should be reduced amongst all the processors in the communicator. </td></tr>
    <tr><td class="paramname">var_glb</td><td>The globally reduced value </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_aff7286de058c49b2a5d7a84be1983e8f_icgraph.svg" width="100%" height="486"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="a398a372625502c0430a64e0bb23e2342"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a398a372625502c0430a64e0bb23e2342">&#9670;&nbsp;</a></span>s_mpi_allreduce_min()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_allreduce_min </td>
          <td>(</td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>var_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>var_glb</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The following subroutine takes the input local variable from all processors and reduces to the minimum of all values. The reduced variable is recorded back onto the original local variable on each processor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">var_loc</td><td>Some variable containing the local value which should be reduced amongst all the processors in the communicator. </td></tr>
    <tr><td class="paramname">var_glb</td><td>The globally reduced value </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a398a372625502c0430a64e0bb23e2342_icgraph.svg" width="100%" height="437"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="a873520062481f3dabcfa77f9bd7728d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a873520062481f3dabcfa77f9bd7728d3">&#9670;&nbsp;</a></span>s_mpi_allreduce_sum()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_allreduce_sum </td>
          <td>(</td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>var_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>var_glb</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">var_loc</td><td>Some variable containing the local value which should be reduced amongst all the processors in the communicator. </td></tr>
    <tr><td class="paramname">var_glb</td><td>The globally reduced value </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a873520062481f3dabcfa77f9bd7728d3_icgraph.svg" width="100%" height="519"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="abfbc42cea69273bc9fa4a2d78f636eb1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abfbc42cea69273bc9fa4a2d78f636eb1">&#9670;&nbsp;</a></span>s_mpi_barrier()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_barrier</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Halts all processes until all have reached barrier. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_abfbc42cea69273bc9fa4a2d78f636eb1_icgraph.svg" width="100%" height="541"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="a69660c5fe9302a8c0496b622fa3b5286"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a69660c5fe9302a8c0496b622fa3b5286">&#9670;&nbsp;</a></span>s_mpi_bcast_user_inputs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_bcast_user_inputs</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Since only the processor with rank 0 reads and verifies the consistency of user inputs, these are initially not available to the other processors. Then, the purpose of this subroutine is to distribute the user inputs to the remaining processors in the communicator. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a69660c5fe9302a8c0496b622fa3b5286_icgraph.svg" width="287" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a80c5e235786545276fe6ffa06965017f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80c5e235786545276fe6ffa06965017f">&#9670;&nbsp;</a></span>s_mpi_decompose_computational_domain()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_decompose_computational_domain</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The purpose of this procedure is to optimally decompose the computational domain among the available processors. This is performed by attempting to award each processor, in each of the coordinate directions, approximately the same number of cells, and then recomputing the affected global parameters. </p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f_cgraph.svg" width="422" height="67"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a80c5e235786545276fe6ffa06965017f_icgraph.svg" width="335" height="67"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a43fbda10c02ec8bc1fc572c83090f2e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a43fbda10c02ec8bc1fc572c83090f2e5">&#9670;&nbsp;</a></span>s_mpi_finalize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_finalize</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The subroutine finalizes the MPI execution environment. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a43fbda10c02ec8bc1fc572c83090f2e5_icgraph.svg" width="287" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a9bc4c617505152d3cc553e5bc25c1ee1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9bc4c617505152d3cc553e5bc25c1ee1">&#9670;&nbsp;</a></span>s_mpi_initialize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_initialize</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The subroutine intializes the MPI execution environment and queries both the number of processors which will be available for the job and the local processor rank. </p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a9bc4c617505152d3cc553e5bc25c1ee1_icgraph.svg" width="287" height="52"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<a id="a340a2c9b1688582ee485dd5d9bbcc864"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a340a2c9b1688582ee485dd5d9bbcc864">&#9670;&nbsp;</a></span>s_mpi_reduce_stability_criteria_extrema()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_reduce_stability_criteria_extrema </td>
          <td>(</td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>icfl_max_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>vcfl_max_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>ccfl_max_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(in)&#160;</td>
          <td class="paramname"><em>Rc_min_loc</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>icfl_max_glb</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>vcfl_max_glb</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>ccfl_max_glb</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">real(kind(0d0)), intent(out)&#160;</td>
          <td class="paramname"><em>Rc_min_glb</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The goal of this subroutine is to determine the global extrema of the stability criteria in the computational domain. This is performed by sifting through the local extrema of each stability criterion. Note that each of the local extrema is from a single process, within its assigned section of the computational domain. Finally, note that the global extrema values are only bookkeept on the rank 0 processor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">icfl_max_loc</td><td>Local maximum ICFL stability criterion </td></tr>
    <tr><td class="paramname">vcfl_max_loc</td><td>Local maximum VCFL stability criterion </td></tr>
    <tr><td class="paramname">ccfl_max_loc</td><td>Local maximum CCFL stability criterion </td></tr>
    <tr><td class="paramname">Rc_min_loc</td><td>Local minimum Rc stability criterion </td></tr>
    <tr><td class="paramname">icfl_max_glb</td><td>Global maximum ICFL stability criterion </td></tr>
    <tr><td class="paramname">vcfl_max_glb</td><td>Global maximum VCFL stability criterion </td></tr>
    <tr><td class="paramname">ccfl_max_glb</td><td>Global maximum CCFL stability criterion </td></tr>
    <tr><td class="paramname">Rc_min_glb</td><td>Global minimum Rc stability criterion </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><div class="zoom"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_a340a2c9b1688582ee485dd5d9bbcc864_icgraph.svg" width="100%" height="437"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>

</div>
</div>
<a id="a9015d32f3d27c21a75e8a916e5090b96"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9015d32f3d27c21a75e8a916e5090b96">&#9670;&nbsp;</a></span>s_mpi_sendrecv_conservative_variables_buffers()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_sendrecv_conservative_variables_buffers </td>
          <td>(</td>
          <td class="paramtype">type(scalar_field), dimension(sys_size), intent(inout)&#160;</td>
          <td class="paramname"><em>q_cons_vf</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>mpi_dir</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>pbc_loc</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The goal of this procedure is to populate the buffers of the cell-average conservative variables by communicating with the neighboring processors. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">q_cons_vf</td><td>Cell-average conservative variables </td></tr>
    <tr><td class="paramname">mpi_dir</td><td>MPI communication coordinate direction </td></tr>
    <tr><td class="paramname">pbc_loc</td><td>Processor boundary condition (PBC) location </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ac348cb6f02f2a9ab70d5c7eac6320231"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac348cb6f02f2a9ab70d5c7eac6320231">&#9670;&nbsp;</a></span>s_mpi_sendrecv_grid_variables_buffers()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">subroutine m_mpi_proxy::s_mpi_sendrecv_grid_variables_buffers </td>
          <td>(</td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>mpi_dir</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>pbc_loc</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The goal of this procedure is to populate the buffers of the grid variables by communicating with the neighboring processors. Note that only the buffers of the cell-width distributions are handled in such a way. This is because the buffers of cell-boundary locations may be calculated directly from those of the cell-width distributions. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">mpi_dir</td><td>MPI communication coordinate direction </td></tr>
    <tr><td class="paramname">pbc_loc</td><td>Processor boundary condition (PBC) location </td></tr>
  </table>
  </dd>
</dl>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="namespacem__mpi__proxy_ac348cb6f02f2a9ab70d5c7eac6320231_icgraph.svg" width="539" height="67"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</div>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="ae5709407e3600d19d79b183e409bb982"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae5709407e3600d19d79b183e409bb982">&#9670;&nbsp;</a></span>err_code</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">integer, private m_mpi_proxy::err_code</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a306ba163b09cfc692125f2c0ba82ef8c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a306ba163b09cfc692125f2c0ba82ef8c">&#9670;&nbsp;</a></span>ierr</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">integer, private m_mpi_proxy::ierr</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

</div>
</div>
<a id="a2915932883654eecbac1283ce03be3ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2915932883654eecbac1283ce03be3ba">&#9670;&nbsp;</a></span>q_cons_buff_recv</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">real(kind(0d0)), dimension(:), allocatable, private m_mpi_proxy::q_cons_buff_recv</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>q_cons_buff_recv is utilized to receive and unpack the buffer of the cell- average conservative variables, for a single computational domain boundary at the time, from the relevant neighboring processor. </p>

</div>
</div>
<a id="a67f9b5b69858f72c61b8d8ebf212567d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67f9b5b69858f72c61b8d8ebf212567d">&#9670;&nbsp;</a></span>q_cons_buff_send</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">real(kind(0d0)), dimension(:), allocatable, private m_mpi_proxy::q_cons_buff_send</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>This variable is utilized to pack and send the buffer of the cell-average conservative variables, for a single computational domain boundary at the time, to the relevant neighboring processor. </p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.9.1-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacem__mpi__proxy.html">m_mpi_proxy</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
